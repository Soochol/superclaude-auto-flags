#!/usr/bin/env python3
"""
SuperClaude Performance Optimizer
ì„±ëŠ¥ ìµœì í™” ë° ë¹ ë¥¸ ì‘ë‹µ ì‹œìŠ¤í…œ
"""

import time
import threading
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class QuickRecommendation:
    """ë¹ ë¥¸ ì¶”ì²œ ê²°ê³¼"""
    flags: str
    confidence: int
    reasoning: str
    response_time_ms: float
    is_enhanced: bool = False
    enhanced_flags: Optional[str] = None
    enhanced_confidence: Optional[int] = None

class PerformanceOptimizer:
    """ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.project_cache = {}
        self.pattern_cache = {}
        self.cache_ttl = 1800  # 30ë¶„
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            'total_requests': 0,
            'cache_hits': 0,
            'average_response_time': 0.0,
            'quick_responses': 0,
            'enhanced_responses': 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ í
        self.background_queue = []
        self.background_thread = None
        self.is_processing = False
    
    def get_quick_recommendation(self, user_input: str, command: str, 
                                description: str, context: Dict[str, Any]) -> QuickRecommendation:
        """ì¦‰ì‹œ ê¸°ë³¸ ì¶”ì²œ ì œê³µ (ëª©í‘œ: <100ms)"""
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"{command}:{hash(description)}:{context.get('project_type', 'unknown')}"
            cached_result = self._get_cached_recommendation(cache_key)
            
            if cached_result:
                self.metrics['cache_hits'] += 1
                response_time = (time.time() - start_time) * 1000
                
                return QuickRecommendation(
                    flags=cached_result['flags'],
                    confidence=cached_result['confidence'],
                    reasoning=f"âš¡ ìºì‹œëœ ì¶”ì²œ (ì‘ë‹µì‹œê°„: {response_time:.1f}ms)",
                    response_time_ms=response_time,
                    is_enhanced=False
                )
            
            # ë¹ ë¥¸ íŒ¨í„´ ë§¤ì¹­
            quick_flags, confidence, reasoning = self._quick_pattern_match(command, description, context)
            
            response_time = (time.time() - start_time) * 1000
            self.metrics['quick_responses'] += 1
            
            # ê²°ê³¼ ìºì‹±
            self._cache_recommendation(cache_key, {
                'flags': quick_flags,
                'confidence': confidence,
                'timestamp': datetime.now()
            })
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í•™ìŠµ ê¸°ë°˜ ê°œì„  ì²˜ë¦¬ ì‹œì‘
            self._queue_background_enhancement(user_input, context, cache_key)
            
            return QuickRecommendation(
                flags=quick_flags,
                confidence=confidence,
                reasoning=f"âš¡ ì¦‰ì‹œ ì¶”ì²œ (ì‘ë‹µì‹œê°„: {response_time:.1f}ms) â€¢ í•™ìŠµ ê°œì„  ì§„í–‰ ì¤‘...",
                response_time_ms=response_time,
                is_enhanced=False
            )
            
        except Exception as e:
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì¶”ì²œ
            response_time = (time.time() - start_time) * 1000
            return QuickRecommendation(
                flags="--think --uc",
                confidence=60,
                reasoning=f"âš¡ ê¸°ë³¸ ì¶”ì²œ (ì˜¤ë¥˜ ë³µêµ¬: {str(e)[:50]}...)",
                response_time_ms=response_time,
                is_enhanced=False
            )
    
    def _quick_pattern_match(self, command: str, description: str, 
                           context: Dict[str, Any]) -> Tuple[str, int, str]:
        """ë¹ ë¥¸ íŒ¨í„´ ë§¤ì¹­ (ë³µì¡í•œ í•™ìŠµ ë¡œì§ ì œì™¸)"""
        
        text = f"{command} {description}".lower()
        
        # ìš°ì„ ìˆœìœ„ íŒ¨í„´ë“¤ (ë¹ ë¥¸ ë§¤ì¹­)
        priority_patterns = {
            'security': {
                'keywords': ['security', 'ë³´ì•ˆ', 'vulnerability', 'ì·¨ì•½ì ', 'audit'],
                'flags': '--persona-security --focus security --validate --uc',
                'confidence': 95
            },
            'performance': {
                'keywords': ['performance', 'ì„±ëŠ¥', 'optimize', 'ìµœì í™”', 'bottleneck'],
                'flags': '--persona-performance --think-hard --focus performance --uc',
                'confidence': 90
            },
            'frontend': {
                'keywords': ['react', 'vue', 'component', 'ì»´í¬ë„ŒíŠ¸', 'ui', 'interface'],
                'flags': '--persona-frontend --magic --c7 --uc',
                'confidence': 94
            },
            'backend': {
                'keywords': ['api', 'server', 'ì„œë²„', 'backend', 'database'],
                'flags': '--persona-backend --seq --c7 --uc',
                'confidence': 92
            },
            'architecture': {
                'keywords': ['architecture', 'ì•„í‚¤í…ì²˜', 'design', 'pattern'],
                'flags': '--persona-architect --ultrathink --seq --uc',
                'confidence': 95
            }
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for pattern_name, pattern_info in priority_patterns.items():
            for keyword in pattern_info['keywords']:
                if keyword in text:
                    # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ì •
                    flags = self._adjust_flags_for_context(pattern_info['flags'], context)
                    
                    return (
                        flags,
                        pattern_info['confidence'],
                        f"ë¹ ë¥¸ íŒ¨í„´ ë§¤ì¹­: {pattern_name}"
                    )
        
        # ê¸°ë³¸ ëª…ë ¹ì–´ë³„ ë§¤ì¹­
        if command == 'analyze':
            flags = '--persona-analyzer --think --uc'
            confidence = 80
        elif command == 'implement':
            if context.get('project_type', '').startswith('python'):
                flags = '--persona-backend --c7 --uc'
            else:
                flags = '--persona-frontend --magic --uc'
            confidence = 75
        elif command == 'improve':
            flags = '--persona-refactorer --loop --uc'
            confidence = 85
        else:
            flags = '--think --uc'
            confidence = 60
        
        # ì»¨í…ìŠ¤íŠ¸ ì¡°ì •
        flags = self._adjust_flags_for_context(flags, context)
        
        return flags, confidence, f"ê¸°ë³¸ ëª…ë ¹ì–´ íŒ¨í„´: {command}"
    
    def _adjust_flags_for_context(self, base_flags: str, context: Dict[str, Any]) -> str:
        """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ í”Œë˜ê·¸ ì¡°ì •"""
        flags = base_flags
        
        # ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        complexity = context.get('complexity', 'moderate')
        if complexity == 'complex' and '--think' in flags and '--think-hard' not in flags:
            flags = flags.replace('--think', '--think-hard')
        
        # íŒŒì¼ ìˆ˜ì— ë”°ë¥¸ delegation ì¶”ê°€
        file_count = context.get('file_count', 0)
        if file_count > 50 and '--delegate' not in flags:
            flags += ' --delegate'
        
        return flags
    
    def _get_cached_recommendation(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ì¶”ì²œ ì¡°íšŒ"""
        if cache_key in self.pattern_cache:
            cached = self.pattern_cache[cache_key]
            # TTL í™•ì¸
            if datetime.now() - cached['timestamp'] < timedelta(seconds=self.cache_ttl):
                return cached
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self.pattern_cache[cache_key]
        
        return None
    
    def _cache_recommendation(self, cache_key: str, recommendation: Dict[str, Any]):
        """ì¶”ì²œ ê²°ê³¼ ìºì‹±"""
        self.pattern_cache[cache_key] = recommendation
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ)
        if len(self.pattern_cache) > 1000:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.pattern_cache.keys(), 
                           key=lambda k: self.pattern_cache[k]['timestamp'])
            del self.pattern_cache[oldest_key]
    
    def _queue_background_enhancement(self, user_input: str, context: Dict[str, Any], cache_key: str):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ê¸°ë°˜ ê°œì„  ì²˜ë¦¬ íì— ì¶”ê°€"""
        enhancement_task = {
            'user_input': user_input,
            'context': context,
            'cache_key': cache_key,
            'timestamp': datetime.now()
        }
        
        self.background_queue.append(enhancement_task)
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘ (ì•„ì§ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë¼ë©´)
        if not self.is_processing:
            self._start_background_processing()
    
    def _start_background_processing(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        if self.background_thread and self.background_thread.is_alive():
            return
        
        self.is_processing = True
        self.background_thread = threading.Thread(target=self._background_processor, daemon=True)
        self.background_thread.start()
    
    def _background_processor(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í•™ìŠµ ê¸°ë°˜ ê°œì„  ì²˜ë¦¬"""
        while self.background_queue or self.is_processing:
            try:
                if not self.background_queue:
                    time.sleep(0.1)
                    continue
                
                task = self.background_queue.pop(0)
                
                # í•™ìŠµ ê¸°ë°˜ ê°œì„  ì²˜ë¦¬ (ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ì‘ì—…)
                enhanced_result = self._process_learning_enhancement(task)
                
                if enhanced_result:
                    # ìºì‹œ ì—…ë°ì´íŠ¸
                    self._update_enhanced_cache(task['cache_key'], enhanced_result)
                    self.metrics['enhanced_responses'] += 1
                
            except Exception as e:
                print(f"Background processing error: {e}")
                continue
        
        self.is_processing = False
    
    def _process_learning_enhancement(self, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """í•™ìŠµ ê¸°ë°˜ ê°œì„  ì²˜ë¦¬ (ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ í˜¸ì¶œ)"""
        try:
            # ì—¬ê¸°ì„œ ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ í˜¸ì¶œ
            # ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ì‘ì—…ì´ë¯€ë¡œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
            
            # ì„ì‹œë¡œ ê°œì„ ëœ ì¶”ì²œ ì‹œë®¬ë ˆì´ì…˜
            time.sleep(1)  # í•™ìŠµ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            
            return {
                'enhanced_flags': task.get('context', {}).get('enhanced_flags', ''),
                'enhanced_confidence': 95,
                'enhancement_reasoning': 'í•™ìŠµ ê¸°ë°˜ ê°œì„  ì™„ë£Œ'
            }
            
        except Exception as e:
            print(f"Learning enhancement error: {e}")
            return None
    
    def _update_enhanced_cache(self, cache_key: str, enhanced_result: Dict[str, Any]):
        """ê°œì„ ëœ ê²°ê³¼ë¡œ ìºì‹œ ì—…ë°ì´íŠ¸"""
        if cache_key in self.pattern_cache:
            self.pattern_cache[cache_key].update(enhanced_result)
            self.pattern_cache[cache_key]['enhanced'] = True
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        self.metrics['cache_hit_rate'] = (
            self.metrics['cache_hits'] / self.metrics['total_requests'] 
            if self.metrics['total_requests'] > 0 else 0
        )
        
        return self.metrics.copy()
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.project_cache.clear()
        self.pattern_cache.clear()
        print("âœ… SuperClaude ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

# ì „ì—­ ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_performance_optimizer = None

def get_performance_optimizer() -> PerformanceOptimizer:
    """ì „ì—­ ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _performance_optimizer
    if _performance_optimizer is None:
        _performance_optimizer = PerformanceOptimizer()
    return _performance_optimizer

def quick_recommend(user_input: str, command: str, description: str, 
                   context: Dict[str, Any]) -> QuickRecommendation:
    """ë¹ ë¥¸ ì¶”ì²œ ì¸í„°í˜ì´ìŠ¤"""
    optimizer = get_performance_optimizer()
    optimizer.metrics['total_requests'] += 1
    
    return optimizer.get_quick_recommendation(user_input, command, description, context)

if __name__ == "__main__":
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    optimizer = PerformanceOptimizer()
    
    test_context = {
        'project_type': 'python_backend',
        'complexity': 'moderate',
        'file_count': 25
    }
    
    print("ğŸš€ SuperClaude ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        ("analyze security vulnerabilities", "analyze", "security vulnerabilities"),
        ("implement React component", "implement", "React component"),
        ("improve performance bottlenecks", "improve", "performance bottlenecks")
    ]
    
    for user_input, command, description in test_cases:
        result = optimizer.get_quick_recommendation(user_input, command, description, test_context)
        
        print(f"\nğŸ“‹ ëª…ë ¹ì–´: {user_input}")
        print(f"âš¡ ì¶”ì²œ í”Œë˜ê·¸: {result.flags}")
        print(f"ğŸ¯ ì‹ ë¢°ë„: {result.confidence}%")
        print(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {result.response_time_ms:.1f}ms")
        print(f"ğŸ’¡ ê·¼ê±°: {result.reasoning}")
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    metrics = optimizer.get_performance_metrics()
    for key, value in metrics.items():
        print(f"   {key}: {value}")