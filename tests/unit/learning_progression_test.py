#!/usr/bin/env python3
"""
SuperClaude Learning Progression Simulation and Test Suite
ÌïôÏäµ ÏãúÏä§ÌÖúÏùò Ïã§Ï†ú ÌïôÏäµ Îä•Î†•Í≥º ÏãúÍ∞Ñ Í≤ΩÍ≥ºÏóê Îî∞Î•∏ Í∞úÏÑ† Ìö®Í≥º Í≤ÄÏ¶ù

This comprehensive test simulates 20-30 user interactions over time to verify:
1. Pattern Recognition Learning
2. User Preference Adaptation
3. Context Awareness Improvement
4. Confidence Calibration Enhancement
5. Feedback Processing Effectiveness
"""

import os
import sys
import json
import random
import sqlite3
import tempfile
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import numpy as np

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from learning_engine import AdaptiveLearningEngine, RecommendationScore
from learning_storage import LearningStorage, UserInteraction, FeedbackRecord

@dataclass
class SimulationScenario:
    """ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÎÇòÎ¶¨Ïò§"""
    name: str
    command: str
    description: str
    project_context: Dict[str, Any]
    expected_pattern: str
    success_probability: float
    execution_time_range: Tuple[float, float]
    user_rating_bias: float  # -1.0 to 1.0

@dataclass
class LearningMetrics:
    """ÌïôÏäµ Ï∏°Ï†ï ÏßÄÌëú"""
    timestamp: str
    interaction_count: int
    avg_confidence: float
    recommendation_accuracy: float
    pattern_diversity: int
    user_preference_strength: float
    context_adaptation_score: float

class LearningProgressionSimulator:
    """ÌïôÏäµ ÏßÑÌñâ ÏãúÎÆ¨Î†àÏù¥ÌÑ∞"""
    
    def __init__(self, temp_dir: str = None):
        # ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú ÌÖåÏä§Ìä∏
        self.temp_dir = temp_dir or tempfile.mkdtemp(prefix="superclaude_learning_test_")
        print(f"üîß ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω ÏÑ§Ï†ï: {self.temp_dir}")
        
        # ÌÖåÏä§Ìä∏Ïö© ÌïôÏäµ ÏãúÏä§ÌÖú ÏÉùÏÑ±
        self.storage = LearningStorage(self.temp_dir)
        self.engine = AdaptiveLearningEngine(self.storage)
        
        # ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÎÇòÎ¶¨Ïò§ Ï†ïÏùò
        self.scenarios = self._create_simulation_scenarios()
        
        # ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        self.user_profile = {
            'security_focus': 0.8,  # Î≥¥ÏïàÏóê Í¥ÄÏã¨ ÎßéÏùå
            'performance_conscious': 0.6,  # ÏÑ±Îä•Ïóê Í¥ÄÏã¨ Î≥¥ÌÜµ
            'frontend_preference': 0.3,  # ÌîÑÎ°†Ìä∏ÏóîÎìú ÏûëÏóÖ Ï†ÅÏùå
            'backend_preference': 0.9,  # Î∞±ÏóîÎìú ÏûëÏóÖ ÎßéÏùå
            'quality_focused': 0.7,  # ÌíàÏßàÏóê Í¥ÄÏã¨ ÎßéÏùå
        }
        
        # ÌîÑÎ°úÏ†ùÌä∏ Ïª®ÌÖçÏä§Ìä∏ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        self.project_contexts = self._create_project_contexts()
        
        # ÌïôÏäµ ÏßÑÌñâ Î©îÌä∏Î¶≠
        self.learning_metrics: List[LearningMetrics] = []
        
        # Ï¥àÍ∏∞ ÏÉÅÌÉú Í∏∞Î°ù
        self._record_initial_metrics()
    
    def _create_simulation_scenarios(self) -> List[SimulationScenario]:
        """ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÎÇòÎ¶¨Ïò§ ÏÉùÏÑ±"""
        return [
            # Î≥¥Ïïà Î∂ÑÏÑù ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="security_vulnerability_analysis",
                command="analyze",
                description="Find security vulnerabilities in authentication system",
                project_context={"languages": ["python"], "frameworks": ["django"], "project_size": "large"},
                expected_pattern="analyze_security",
                success_probability=0.85,
                execution_time_range=(45, 90),
                user_rating_bias=0.3
            ),
            SimulationScenario(
                name="api_security_audit",
                command="analyze",
                description="Security audit for REST API endpoints",
                project_context={"languages": ["javascript"], "frameworks": ["express"], "project_size": "medium"},
                expected_pattern="analyze_security",
                success_probability=0.80,
                execution_time_range=(30, 60),
                user_rating_bias=0.2
            ),
            
            # ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="database_performance_optimization",
                command="improve",
                description="Optimize database query performance",
                project_context={"languages": ["python"], "frameworks": ["sqlalchemy"], "project_size": "large"},
                expected_pattern="improve_performance",
                success_probability=0.75,
                execution_time_range=(60, 120),
                user_rating_bias=0.1
            ),
            SimulationScenario(
                name="frontend_performance_tuning",
                command="improve",
                description="Improve React component rendering performance",
                project_context={"languages": ["javascript"], "frameworks": ["react"], "project_size": "medium"},
                expected_pattern="improve_performance",
                success_probability=0.70,
                execution_time_range=(40, 80),
                user_rating_bias=-0.1
            ),
            
            # UI Íµ¨ÌòÑ ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="react_component_implementation",
                command="implement",
                description="Create responsive dashboard component",
                project_context={"languages": ["javascript"], "frameworks": ["react"], "project_size": "medium"},
                expected_pattern="implement_ui",
                success_probability=0.90,
                execution_time_range=(25, 50),
                user_rating_bias=0.0
            ),
            SimulationScenario(
                name="vue_form_component",
                command="implement",
                description="Build complex form component with validation",
                project_context={"languages": ["javascript"], "frameworks": ["vue"], "project_size": "small"},
                expected_pattern="implement_ui",
                success_probability=0.85,
                execution_time_range=(30, 60),
                user_rating_bias=0.1
            ),
            
            # API Íµ¨ÌòÑ ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="rest_api_implementation",
                command="implement",
                description="Implement user authentication API endpoints",
                project_context={"languages": ["python"], "frameworks": ["fastapi"], "project_size": "medium"},
                expected_pattern="implement_api",
                success_probability=0.85,
                execution_time_range=(50, 100),
                user_rating_bias=0.2
            ),
            SimulationScenario(
                name="graphql_api_implementation",
                command="implement",
                description="Create GraphQL resolvers for user management",
                project_context={"languages": ["javascript"], "frameworks": ["apollo"], "project_size": "large"},
                expected_pattern="implement_api",
                success_probability=0.80,
                execution_time_range=(60, 120),
                user_rating_bias=0.1
            ),
            
            # ÏïÑÌÇ§ÌÖçÏ≤ò Î∂ÑÏÑù ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="microservices_architecture_review",
                command="analyze",
                description="Review microservices architecture design",
                project_context={"languages": ["python", "javascript"], "frameworks": ["docker", "kubernetes"], "project_size": "very_large"},
                expected_pattern="analyze_architecture",
                success_probability=0.75,
                execution_time_range=(90, 180),
                user_rating_bias=0.3
            ),
            
            # ÏΩîÎìú ÌíàÏßà Í∞úÏÑ† ÏãúÎÇòÎ¶¨Ïò§
            SimulationScenario(
                name="code_quality_improvement",
                command="improve",
                description="Refactor legacy code for better maintainability",
                project_context={"languages": ["python"], "frameworks": [], "project_size": "large"},
                expected_pattern="improve_quality",
                success_probability=0.80,
                execution_time_range=(70, 140),
                user_rating_bias=0.2
            ),
        ]
    
    def _create_project_contexts(self) -> List[Dict[str, Any]]:
        """ÌîÑÎ°úÏ†ùÌä∏ Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        return [
            {
                "name": "enterprise_auth_system",
                "languages": ["python"],
                "frameworks": ["django", "celery"],
                "project_size": "very_large",
                "file_count": 150,
                "security_sensitive": True
            },
            {
                "name": "ecommerce_frontend",
                "languages": ["javascript", "typescript"],
                "frameworks": ["react", "nextjs"],
                "project_size": "large",
                "file_count": 80,
                "performance_critical": True
            },
            {
                "name": "api_gateway",
                "languages": ["python"],
                "frameworks": ["fastapi", "redis"],
                "project_size": "medium",
                "file_count": 45,
                "high_availability": True
            },
            {
                "name": "mobile_app_backend",
                "languages": ["javascript"],
                "frameworks": ["express", "mongodb"],
                "project_size": "medium",
                "file_count": 35,
                "mobile_optimized": True
            },
            {
                "name": "data_pipeline",
                "languages": ["python"],
                "frameworks": ["airflow", "pandas"],
                "project_size": "large",
                "file_count": 65,
                "data_intensive": True
            }
        ]
    
    def _record_initial_metrics(self):
        """Ï¥àÍ∏∞ Î©îÌä∏Î¶≠ Í∏∞Î°ù"""
        metrics = LearningMetrics(
            timestamp=datetime.now().isoformat(),
            interaction_count=0,
            avg_confidence=0.0,
            recommendation_accuracy=0.0,
            pattern_diversity=0,
            user_preference_strength=0.0,
            context_adaptation_score=0.0
        )
        self.learning_metrics.append(metrics)
    
    def simulate_user_interactions(self, num_interactions: int = 25) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö©Ïûê ÏÉÅÌò∏ÏûëÏö© ÏãúÎÆ¨Î†àÏù¥ÏÖò"""
        print(f"\nüéØ {num_interactions}Í∞ú ÏÇ¨Ïö©Ïûê ÏÉÅÌò∏ÏûëÏö© ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÏûë")
        
        interactions = []
        
        for i in range(num_interactions):
            # ÏãúÎÇòÎ¶¨Ïò§ ÏÑ†ÌÉù (ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ Í∏∞Î∞ò Í∞ÄÏ§ë ÏÑ†ÌÉù)
            scenario = self._select_scenario_by_user_preference()
            project_context = random.choice(self.project_contexts)
            
            # ÏãúÍ∞Ñ Í≤ΩÍ≥º ÏãúÎÆ¨Î†àÏù¥ÏÖò (1-7Ïùº Í∞ÑÍ≤©)
            if i > 0:
                time_gap = random.uniform(1, 7)  # 1-7Ïùº
                time.sleep(0.1)  # Ïã§Ï†ú ÏãúÍ∞Ñ Í∞ÑÍ≤© ÏãúÎÆ¨Î†àÏù¥ÏÖò
            
            print(f"  üìã ÏÉÅÌò∏ÏûëÏö© {i+1}/{num_interactions}: {scenario.name}")
            
            # Ï∂îÏ≤ú ÏÉùÏÑ±
            recommendation = self.engine.get_adaptive_recommendation(
                scenario.command,
                scenario.description,
                project_context
            )
            
            # ÏÑ±Í≥µ/Ïã§Ìå® ÏãúÎÆ¨Î†àÏù¥ÏÖò
            success = random.random() < scenario.success_probability
            execution_time = random.uniform(*scenario.execution_time_range)
            
            # ÏÇ¨Ïö©Ïûê ÌèâÏ†ê ÏãúÎÆ¨Î†àÏù¥ÏÖò
            base_rating = 4 if success else 2
            rating_adjustment = scenario.user_rating_bias * random.uniform(-1, 1)
            user_rating = max(1, min(5, int(base_rating + rating_adjustment)))
            
            # ÏÉÅÌò∏ÏûëÏö© Í∏∞Î°ù
            interaction = UserInteraction(
                timestamp=datetime.now().isoformat(),
                user_input=f"/sc:{scenario.command} {scenario.description}",
                command=scenario.command,
                description=scenario.description,
                recommended_flags=recommendation.flags,
                actual_flags=recommendation.flags,  # ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóêÏÑúÎäî ÎèôÏùº
                project_context=project_context,
                success=success,
                execution_time=execution_time,
                confidence=recommendation.confidence,
                reasoning=json.dumps(recommendation.reasoning),
                user_id=self.storage.user_id,
                project_hash=self.storage.get_project_hash(project_context['name'])
            )
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Í∏∞Î°ù
            interaction_id = self.storage.record_interaction(interaction)
            
            # ÌîºÎìúÎ∞± Í∏∞Î°ù
            feedback = FeedbackRecord(
                timestamp=datetime.now().isoformat(),
                interaction_id=interaction_id,
                feedback_type="implicit",
                rating=user_rating,
                success_indicator=success,
                user_correction=None,
                user_id=self.storage.user_id,
                project_hash=self.storage.get_project_hash(project_context['name'])
            )
            
            self.storage.record_feedback(feedback)
            
            # ÌïôÏäµ ÏóîÏßÑÏóê ÌîºÎìúÎ∞± Ï†ÅÏö©
            self.engine.update_learning_from_feedback(
                interaction_id, success, execution_time, user_rating
            )
            
            # Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌïôÏäµ ÏàòÌñâ Î∞è Î©îÌä∏Î¶≠ Í∏∞Î°ù
            if (i + 1) % 5 == 0:
                print(f"    üß† ÌïôÏäµ ÏßÑÌñâ Ï§ë... ({i+1}Í∞ú ÏÉÅÌò∏ÏûëÏö© ÏôÑÎ£å)")
                self.engine.learn_from_interactions(days=30)
                self._record_learning_metrics(i + 1)
            
            interactions.append({
                'scenario': scenario.name,
                'recommendation': asdict(recommendation),
                'success': success,
                'execution_time': execution_time,
                'user_rating': user_rating,
                'interaction_id': interaction_id
            })
        
        # ÏµúÏ¢Ö ÌïôÏäµ ÏàòÌñâ
        print("  üéì ÏµúÏ¢Ö ÌïôÏäµ ÏàòÌñâ Ï§ë...")
        final_learning_stats = self.engine.learn_from_interactions(days=30)
        self._record_learning_metrics(num_interactions)
        
        print(f"‚úÖ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å: {num_interactions}Í∞ú ÏÉÅÌò∏ÏûëÏö©, ÌïôÏäµ ÌÜµÍ≥Ñ: {final_learning_stats}")
        return interactions
    
    def _select_scenario_by_user_preference(self) -> SimulationScenario:
        """ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Í∏∞Î∞ò ÏãúÎÇòÎ¶¨Ïò§ ÏÑ†ÌÉù"""
        weights = []
        
        for scenario in self.scenarios:
            weight = 1.0  # Í∏∞Î≥∏ Í∞ÄÏ§ëÏπò
            
            # ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Ï°∞Ï†ï
            if 'security' in scenario.name:
                weight *= (1 + self.user_profile['security_focus'])
            elif 'performance' in scenario.name:
                weight *= (1 + self.user_profile['performance_conscious'])
            elif 'ui' in scenario.name or 'component' in scenario.name:
                weight *= (1 + self.user_profile['frontend_preference'])
            elif 'api' in scenario.name:
                weight *= (1 + self.user_profile['backend_preference'])
            elif 'quality' in scenario.name:
                weight *= (1 + self.user_profile['quality_focused'])
            
            weights.append(weight)
        
        # Í∞ÄÏ§ë ÎûúÎç§ ÏÑ†ÌÉù
        total_weight = sum(weights)
        normalized_weights = [w/total_weight for w in weights]
        
        return np.random.choice(self.scenarios, p=normalized_weights)
    
    def _record_learning_metrics(self, interaction_count: int):
        """ÌïôÏäµ Î©îÌä∏Î¶≠ Í∏∞Î°ù"""
        # ÏµúÍ∑º ÏÉÅÌò∏ÏûëÏö© Î∂ÑÏÑù
        recent_interactions = self.storage.get_user_interactions(days=30)
        
        if not recent_interactions:
            return
        
        # ÌèâÍ∑† Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        avg_confidence = np.mean([i['confidence'] for i in recent_interactions])
        
        # Ï∂îÏ≤ú Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ (ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ Ï∂îÏ≤úÏù¥ ÏÑ±Í≥µÌñàÎäîÏßÄ)
        high_confidence_interactions = [i for i in recent_interactions if i['confidence'] >= 80]
        if high_confidence_interactions:
            accurate_predictions = sum(1 for i in high_confidence_interactions if i['success'])
            recommendation_accuracy = accurate_predictions / len(high_confidence_interactions)
        else:
            recommendation_accuracy = 0.0
        
        # Ìå®ÌÑ¥ Îã§ÏñëÏÑ± (ÌïôÏäµÎêú Ìå®ÌÑ¥ Ïàò)
        pattern_success_rates = self.storage.get_pattern_success_rates()
        pattern_diversity = len(pattern_success_rates)
        
        # ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Í∞ïÎèÑ (ÏÑ†Ìò∏ÎèÑ Ìé∏Ï∞® ÌèâÍ∑†)
        user_preferences = self.storage.get_user_preferences()
        if user_preferences:
            preference_values = list(user_preferences.values())
            user_preference_strength = np.std(preference_values)  # Ìé∏Ï∞®Í∞Ä ÌÅ¥ÏàòÎ°ù ÏÑ†Ìò∏ÎèÑÍ∞Ä ÎöúÎ†∑Ìï®
        else:
            user_preference_strength = 0.0
        
        # Ïª®ÌÖçÏä§Ìä∏ Ï†ÅÏùë Ï†êÏàò (Ïª®ÌÖçÏä§Ìä∏Î≥Ñ ÏÑ±Í≥µÎ•† Ï∞®Ïù¥)
        context_adaptation_score = self._calculate_context_adaptation_score(recent_interactions)
        
        metrics = LearningMetrics(
            timestamp=datetime.now().isoformat(),
            interaction_count=interaction_count,
            avg_confidence=avg_confidence,
            recommendation_accuracy=recommendation_accuracy,
            pattern_diversity=pattern_diversity,
            user_preference_strength=user_preference_strength,
            context_adaptation_score=context_adaptation_score
        )
        
        self.learning_metrics.append(metrics)
    
    def _calculate_context_adaptation_score(self, interactions: List[Dict]) -> float:
        """Ïª®ÌÖçÏä§Ìä∏ Ï†ÅÏùë Ï†êÏàò Í≥ÑÏÇ∞"""
        if len(interactions) < 10:
            return 0.0
        
        # ÌîÑÎ°úÏ†ùÌä∏Î≥Ñ ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
        project_success_rates = {}
        for interaction in interactions:
            project_hash = interaction['project_hash']
            if project_hash not in project_success_rates:
                project_success_rates[project_hash] = []
            project_success_rates[project_hash].append(interaction['success'])
        
        # Í∞Å ÌîÑÎ°úÏ†ùÌä∏Ïùò ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
        success_rates = []
        for project_hash, successes in project_success_rates.items():
            if len(successes) >= 3:  # ÏµúÏÜå 3Í∞ú ÏÉòÌîå
                success_rate = sum(successes) / len(successes)
                success_rates.append(success_rate)
        
        if len(success_rates) < 2:
            return 0.0
        
        # ÏÑ±Í≥µÎ•† Î∂ÑÏÇ∞ Í≥ÑÏÇ∞ (ÎÇÆÏùÑÏàòÎ°ù ÏùºÍ¥ÄÏÑ± ÏûàÏùå = Ï¢ãÏùÄ Ï†ÅÏùë)
        variance = np.var(success_rates)
        adaptation_score = max(0.0, 1.0 - variance * 2)  # 0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî
        
        return adaptation_score
    
    def test_learning_mechanisms(self) -> Dict[str, Any]:
        """ÌïôÏäµ Î©îÏª§ÎãàÏ¶ò ÌÖåÏä§Ìä∏"""
        print("\nüî¨ ÌïôÏäµ Î©îÏª§ÎãàÏ¶ò ÌÖåÏä§Ìä∏ ÏãúÏûë")
        
        results = {}
        
        # 1. Ìå®ÌÑ¥ Ïù∏Ïãù ÌÖåÏä§Ìä∏
        print("  üìä Ìå®ÌÑ¥ Ïù∏Ïãù Îä•Î†• ÌÖåÏä§Ìä∏")
        pattern_recognition_score = self._test_pattern_recognition()
        results['pattern_recognition'] = pattern_recognition_score
        
        # 2. ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Ï†ÅÏùë ÌÖåÏä§Ìä∏
        print("  üë§ ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Ï†ÅÏùë ÌÖåÏä§Ìä∏")
        user_adaptation_score = self._test_user_preference_adaptation()
        results['user_preference_adaptation'] = user_adaptation_score
        
        # 3. Ïª®ÌÖçÏä§Ìä∏ Ïù∏Ïãù ÌÖåÏä§Ìä∏
        print("  üéØ Ïª®ÌÖçÏä§Ìä∏ Ïù∏Ïãù Îä•Î†• ÌÖåÏä§Ìä∏")
        context_awareness_score = self._test_context_awareness()
        results['context_awareness'] = context_awareness_score
        
        # 4. Ïã†Î¢∞ÎèÑ Î≥¥Ï†ï ÌÖåÏä§Ìä∏
        print("  üìà Ïã†Î¢∞ÎèÑ Î≥¥Ï†ï Îä•Î†• ÌÖåÏä§Ìä∏")
        confidence_calibration_score = self._test_confidence_calibration()
        results['confidence_calibration'] = confidence_calibration_score
        
        # 5. ÌîºÎìúÎ∞± Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏
        print("  üîÑ ÌîºÎìúÎ∞± Ï≤òÎ¶¨ Ìö®Í≥º ÌÖåÏä§Ìä∏")
        feedback_effectiveness_score = self._test_feedback_processing()
        results['feedback_processing'] = feedback_effectiveness_score
        
        return results
    
    def _test_pattern_recognition(self) -> float:
        """Ìå®ÌÑ¥ Ïù∏Ïãù Îä•Î†• ÌÖåÏä§Ìä∏"""
        # ÏïåÎ†§ÏßÑ Ìå®ÌÑ¥Ïóê ÎåÄÌïú Ï∂îÏ≤ú Ï†ïÌôïÎèÑ Ï∏°Ï†ï
        test_cases = [
            ("analyze", "security vulnerability scan", "analyze_security"),
            ("implement", "React dashboard component", "implement_ui"),
            ("improve", "database query performance", "improve_performance"),
            ("analyze", "system architecture review", "analyze_architecture"),
        ]
        
        correct_predictions = 0
        
        for command, description, expected_pattern in test_cases:
            project_context = random.choice(self.project_contexts)
            recommendation = self.engine.get_adaptive_recommendation(command, description, project_context)
            
            # Ï∂îÏ≤úÎêú ÌîåÎûòÍ∑∏Í∞Ä ÏòàÏÉÅ Ìå®ÌÑ¥Í≥º ÏùºÏπòÌïòÎäîÏßÄ ÌôïÏù∏
            expected_flags = self.engine._get_pattern_base_flags(expected_pattern)
            
            # Ï£ºÏöî ÌîåÎûòÍ∑∏Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏
            expected_flag_list = expected_flags.split()
            recommended_flag_list = recommendation.flags.split()
            
            # ÌïµÏã¨ ÌîåÎûòÍ∑∏ Îß§Ïπ≠ ÌôïÏù∏
            core_match = any(flag in recommended_flag_list for flag in expected_flag_list[:2])
            
            if core_match:
                correct_predictions += 1
        
        return correct_predictions / len(test_cases)
    
    def _test_user_preference_adaptation(self) -> float:
        """ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Ï†ÅÏùë ÌÖåÏä§Ìä∏"""
        # ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Ï°∞Ìöå
        user_preferences = self.storage.get_user_preferences()
        
        if not user_preferences:
            return 0.0
        
        # ÏÑ†Ìò∏ÎèÑ Î∂ÑÏÇ∞ Ï∏°Ï†ï (ÎÜíÏùÑÏàòÎ°ù Í∞úÏù∏ÌôîÍ∞Ä Ïûò Îê®)
        preference_values = list(user_preferences.values())
        preference_variance = np.var(preference_values)
        
        # 0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî
        adaptation_score = min(1.0, preference_variance)
        
        return adaptation_score
    
    def _test_context_awareness(self) -> float:
        """Ïª®ÌÖçÏä§Ìä∏ Ïù∏Ïãù Îä•Î†• ÌÖåÏä§Ìä∏"""
        # ÎèôÏùºÌïú Î™ÖÎ†πÏñ¥, Îã§Î•∏ Ïª®ÌÖçÏä§Ìä∏Ïóê ÎåÄÌïú Ï∂îÏ≤ú Ï∞®Ïù¥ Ï∏°Ï†ï
        test_command = "implement"
        test_description = "user authentication system"
        
        contexts = [
            {"languages": ["python"], "frameworks": ["django"], "project_size": "large"},
            {"languages": ["javascript"], "frameworks": ["react"], "project_size": "small"},
            {"languages": ["python"], "frameworks": ["fastapi"], "project_size": "medium"}
        ]
        
        recommendations = []
        for context in contexts:
            rec = self.engine.get_adaptive_recommendation(test_command, test_description, context)
            recommendations.append(rec.flags)
        
        # Ï∂îÏ≤úÏùò Îã§ÏñëÏÑ± Ï∏°Ï†ï (Îã§Î•∏ Ïª®ÌÖçÏä§Ìä∏Ïóê Îã§Î•∏ Ï∂îÏ≤ú)
        unique_recommendations = len(set(recommendations))
        context_awareness_score = unique_recommendations / len(contexts)
        
        return context_awareness_score
    
    def _test_confidence_calibration(self) -> float:
        """Ïã†Î¢∞ÎèÑ Î≥¥Ï†ï Îä•Î†• ÌÖåÏä§Ìä∏"""
        # ÏµúÍ∑º ÏÉÅÌò∏ÏûëÏö©ÏóêÏÑú Ïã†Î¢∞ÎèÑÏôÄ Ïã§Ï†ú ÏÑ±Í≥µÎ•†Ïùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Ï∏°Ï†ï
        recent_interactions = self.storage.get_user_interactions(days=30)
        
        if len(recent_interactions) < 10:
            return 0.0
        
        # Ïã†Î¢∞ÎèÑ Íµ¨Í∞ÑÎ≥Ñ Ïã§Ï†ú ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
        high_confidence = [i for i in recent_interactions if i['confidence'] >= 80]
        medium_confidence = [i for i in recent_interactions if 60 <= i['confidence'] < 80]
        low_confidence = [i for i in recent_interactions if i['confidence'] < 60]
        
        confidence_accuracy = 0.0
        weight_sum = 0.0
        
        # Í≥†Ïã†Î¢∞ÎèÑ Íµ¨Í∞Ñ (80% Ïù¥ÏÉÅ ÏÑ±Í≥µÌï¥Ïïº Ìï®)
        if high_confidence:
            high_success_rate = sum(i['success'] for i in high_confidence) / len(high_confidence)
            if high_success_rate >= 0.75:  # Í∏∞ÎåÄÍ∞íÎ≥¥Îã§ ÎÜíÏúºÎ©¥ Ï†êÏàò
                confidence_accuracy += high_success_rate * 0.5
            weight_sum += 0.5
        
        # Ï§ëÍ∞ÑÏã†Î¢∞ÎèÑ Íµ¨Í∞Ñ (60-70% ÏÑ±Í≥µ)
        if medium_confidence:
            medium_success_rate = sum(i['success'] for i in medium_confidence) / len(medium_confidence)
            if 0.55 <= medium_success_rate <= 0.75:  # Ï†ÅÏ†àÌïú Î≤îÏúÑ
                confidence_accuracy += 0.65 * 0.3
            weight_sum += 0.3
        
        # Ï†ÄÏã†Î¢∞ÎèÑ Íµ¨Í∞Ñ (50% Ïù¥Ìïò ÏÑ±Í≥µ)
        if low_confidence:
            low_success_rate = sum(i['success'] for i in low_confidence) / len(low_confidence)
            if low_success_rate <= 0.6:  # ÏòàÏÉÅÎåÄÎ°ú ÎÇÆÏùÄ ÏÑ±Í≥µÎ•†
                confidence_accuracy += (1 - low_success_rate) * 0.2
            weight_sum += 0.2
        
        return confidence_accuracy / weight_sum if weight_sum > 0 else 0.0
    
    def _test_feedback_processing(self) -> float:
        """ÌîºÎìúÎ∞± Ï≤òÎ¶¨ Ìö®Í≥º ÌÖåÏä§Ìä∏"""
        # ÏãúÍ∞Ñ Í≤ΩÍ≥ºÏóê Îî∞Î•∏ ÏÑ±Í≥µÎ•† Í∞úÏÑ† Ï∏°Ï†ï
        all_interactions = self.storage.get_user_interactions(days=30)
        
        if len(all_interactions) < 20:
            return 0.0
        
        # ÏãúÍ∞ÑÏàú Ï†ïÎ†¨
        sorted_interactions = sorted(all_interactions, key=lambda x: x['timestamp'])
        
        # Ï¥àÍ∏∞ Ï†àÎ∞òÍ≥º ÌõÑÎ∞ò Ï†àÎ∞òÏùò ÏÑ±Í≥µÎ•† ÎπÑÍµê
        half_point = len(sorted_interactions) // 2
        
        early_interactions = sorted_interactions[:half_point]
        late_interactions = sorted_interactions[half_point:]
        
        early_success_rate = sum(i['success'] for i in early_interactions) / len(early_interactions)
        late_success_rate = sum(i['success'] for i in late_interactions) / len(late_interactions)
        
        # Í∞úÏÑ† Ï†ïÎèÑ Ï∏°Ï†ï (0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî)
        improvement = late_success_rate - early_success_rate
        improvement_score = max(0.0, min(1.0, improvement + 0.5))  # -0.5 ~ 0.5 -> 0 ~ 1
        
        return improvement_score
    
    def measure_learning_effectiveness(self) -> Dict[str, Any]:
        """ÌïôÏäµ Ìö®Í≥º Ï∏°Ï†ï"""
        print("\nüìä ÌïôÏäµ Ìö®Í≥º Ï∏°Ï†ï Ï§ë...")
        
        if len(self.learning_metrics) < 2:
            return {"error": "Ï∂©Î∂ÑÌïú Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"}
        
        initial_metrics = self.learning_metrics[0]
        final_metrics = self.learning_metrics[-1]
        
        # Í∞úÏÑ† ÏßÄÌëú Í≥ÑÏÇ∞
        improvements = {
            'confidence_improvement': final_metrics.avg_confidence - initial_metrics.avg_confidence,
            'accuracy_improvement': final_metrics.recommendation_accuracy - initial_metrics.recommendation_accuracy,
            'pattern_diversity_growth': final_metrics.pattern_diversity - initial_metrics.pattern_diversity,
            'preference_strength_growth': final_metrics.user_preference_strength - initial_metrics.user_preference_strength,
            'context_adaptation_improvement': final_metrics.context_adaptation_score - initial_metrics.context_adaptation_score
        }
        
        # Ï†ÑÏ≤¥ ÌïôÏäµ Ìö®Í≥º Ï†êÏàò
        learning_effectiveness_score = np.mean([
            max(0, improvements['confidence_improvement'] / 20),  # 20Ï†ê Í∞úÏÑ† = 1.0Ï†ê
            max(0, improvements['accuracy_improvement']),  # ÏßÅÏ†ë ÎπÑÏú®
            max(0, improvements['pattern_diversity_growth'] / 10),  # 10Í∞ú Ìå®ÌÑ¥ = 1.0Ï†ê
            max(0, improvements['preference_strength_growth']),  # ÏßÅÏ†ë Ï†êÏàò
            max(0, improvements['context_adaptation_improvement'])  # ÏßÅÏ†ë Ï†êÏàò
        ])
        
        return {
            'initial_state': asdict(initial_metrics),
            'final_state': asdict(final_metrics),
            'improvements': improvements,
            'learning_effectiveness_score': learning_effectiveness_score,
            'metrics_timeline': [asdict(m) for m in self.learning_metrics]
        }
    
    def test_data_persistence(self) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ ÏßÄÏÜçÏÑ± ÌÖåÏä§Ìä∏"""
        print("\nüíæ Îç∞Ïù¥ÌÑ∞ ÏßÄÏÜçÏÑ± ÌÖåÏä§Ìä∏")
        
        # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞ ÏÉÅÌÉú Í∏∞Î°ù
        before_state = {
            'interactions_count': len(self.storage.get_user_interactions(days=30)),
            'patterns_count': len(self.storage.get_pattern_success_rates()),
            'preferences_count': len(self.storage.get_user_preferences())
        }
        
        # ÏÉàÎ°úÏö¥ Ïä§ÌÜ†Î¶¨ÏßÄ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (Ïû¨ÏãúÏûë ÏãúÎÆ¨Î†àÏù¥ÏÖò)
        new_storage = LearningStorage(self.temp_dir)
        new_engine = AdaptiveLearningEngine(new_storage)
        
        # Îç∞Ïù¥ÌÑ∞ Î≥µÍµ¨ ÌôïÏù∏
        after_state = {
            'interactions_count': len(new_storage.get_user_interactions(days=30)),
            'patterns_count': len(new_storage.get_pattern_success_rates()),
            'preferences_count': len(new_storage.get_user_preferences())
        }
        
        # Îç∞Ïù¥ÌÑ∞ ÏùºÍ¥ÄÏÑ± ÌôïÏù∏
        data_consistency = {
            'interactions_preserved': before_state['interactions_count'] == after_state['interactions_count'],
            'patterns_preserved': before_state['patterns_count'] == after_state['patterns_count'],
            'preferences_preserved': before_state['preferences_count'] == after_state['preferences_count']
        }
        
        # ÌïôÏäµ ÏÉÅÌÉú Î≥µÍµ¨ ÌÖåÏä§Ìä∏
        test_recommendation = new_engine.get_adaptive_recommendation(
            "analyze", "security vulnerability assessment", 
            {"languages": ["python"], "project_size": "large"}
        )
        
        learning_state_recovered = test_recommendation.confidence > 60  # Í∏∞Î≥∏Í∞íÎ≥¥Îã§ ÎÜíÏïÑÏïº Ìï®
        
        return {
            'before_restart': before_state,
            'after_restart': after_state,
            'data_consistency': data_consistency,
            'learning_state_recovered': learning_state_recovered,
            'persistence_score': sum(data_consistency.values()) / len(data_consistency)
        }
    
    def compare_cold_vs_warm_performance(self) -> Dict[str, Any]:
        """ÏΩúÎìú Ïä§ÌÉÄÌä∏ vs Ïõú ÏãúÏä§ÌÖú ÏÑ±Îä• ÎπÑÍµê"""
        print("\nüå°Ô∏è ÏΩúÎìú Ïä§ÌÉÄÌä∏ vs Ïõú ÏãúÏä§ÌÖú ÏÑ±Îä• ÎπÑÍµê")
        
        # Ïõú ÏãúÏä§ÌÖú ÏÑ±Îä• (ÌòÑÏû¨ ÌïôÏäµÎêú ÏÉÅÌÉú)
        warm_test_cases = [
            ("analyze", "security audit for web application"),
            ("implement", "REST API for user management"),
            ("improve", "database query optimization"),
        ]
        
        warm_performance = []
        for command, description in warm_test_cases:
            context = random.choice(self.project_contexts)
            recommendation = self.engine.get_adaptive_recommendation(command, description, context)
            warm_performance.append({
                'confidence': recommendation.confidence,
                'flags_count': len(recommendation.flags.split()),
                'reasoning_depth': len(recommendation.reasoning)
            })
        
        # ÏΩúÎìú ÏãúÏä§ÌÖú ÏÑ±Îä• (ÏÉàÎ°úÏö¥ ÏûÑÏãú ÏãúÏä§ÌÖú)
        cold_temp_dir = tempfile.mkdtemp(prefix="superclaude_cold_test_")
        cold_storage = LearningStorage(cold_temp_dir)
        cold_engine = AdaptiveLearningEngine(cold_storage)
        
        cold_performance = []
        for command, description in warm_test_cases:
            context = random.choice(self.project_contexts)
            recommendation = cold_engine.get_adaptive_recommendation(command, description, context)
            cold_performance.append({
                'confidence': recommendation.confidence,
                'flags_count': len(recommendation.flags.split()),
                'reasoning_depth': len(recommendation.reasoning)
            })
        
        # ÏÑ±Îä• Ï∞®Ïù¥ Í≥ÑÏÇ∞
        warm_avg_confidence = np.mean([p['confidence'] for p in warm_performance])
        cold_avg_confidence = np.mean([p['confidence'] for p in cold_performance])
        
        warm_avg_flags = np.mean([p['flags_count'] for p in warm_performance])
        cold_avg_flags = np.mean([p['flags_count'] for p in cold_performance])
        
        warm_avg_reasoning = np.mean([p['reasoning_depth'] for p in warm_performance])
        cold_avg_reasoning = np.mean([p['reasoning_depth'] for p in cold_performance])
        
        # Ï†ïÎ¶¨
        import shutil
        shutil.rmtree(cold_temp_dir, ignore_errors=True)
        
        return {
            'warm_system': {
                'avg_confidence': warm_avg_confidence,
                'avg_flags_count': warm_avg_flags,
                'avg_reasoning_depth': warm_avg_reasoning,
                'performance_samples': warm_performance
            },
            'cold_system': {
                'avg_confidence': cold_avg_confidence,
                'avg_flags_count': cold_avg_flags,
                'avg_reasoning_depth': cold_avg_reasoning,
                'performance_samples': cold_performance
            },
            'improvements': {
                'confidence_gain': warm_avg_confidence - cold_avg_confidence,
                'flags_sophistication_gain': warm_avg_flags - cold_avg_flags,
                'reasoning_depth_gain': warm_avg_reasoning - cold_avg_reasoning
            },
            'overall_improvement_score': (
                (warm_avg_confidence - cold_avg_confidence) / 100 +  # Ïã†Î¢∞ÎèÑ Í∞úÏÑ†
                max(0, (warm_avg_flags - cold_avg_flags) / 5) +  # ÌîåÎûòÍ∑∏ Ï†ïÍµêÌï®
                max(0, (warm_avg_reasoning - cold_avg_reasoning) / 10)  # Ï∂îÎ°† ÍπäÏù¥
            ) / 3
        }
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Ï¢ÖÌï© ÌÖåÏä§Ìä∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        print("\nüìã Ï¢ÖÌï© ÌÖåÏä§Ìä∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï§ë...")
        
        # Î™®Îì† ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏàòÏßë
        learning_mechanisms = self.test_learning_mechanisms()
        learning_effectiveness = self.measure_learning_effectiveness()
        data_persistence = self.test_data_persistence()
        cold_vs_warm = self.compare_cold_vs_warm_performance()
        
        # ÏµúÏ¢Ö ÌïôÏäµ Î∂ÑÏÑù
        learning_analysis = self.engine.analyze_learning_progress()
        
        # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
        overall_scores = {
            'pattern_recognition': learning_mechanisms.get('pattern_recognition', 0),
            'user_adaptation': learning_mechanisms.get('user_preference_adaptation', 0),
            'context_awareness': learning_mechanisms.get('context_awareness', 0),
            'confidence_calibration': learning_mechanisms.get('confidence_calibration', 0),
            'feedback_processing': learning_mechanisms.get('feedback_processing', 0),
            'learning_effectiveness': learning_effectiveness.get('learning_effectiveness_score', 0),
            'data_persistence': data_persistence.get('persistence_score', 0),
            'warm_vs_cold_improvement': cold_vs_warm.get('overall_improvement_score', 0)
        }
        
        total_score = np.mean(list(overall_scores.values()))
        
        # Í≤∞Î°† ÎèÑÏ∂ú
        conclusions = []
        if total_score >= 0.8:
            conclusions.append("‚úÖ ÌïôÏäµ ÏãúÏä§ÌÖúÏù¥ Îß§Ïö∞ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏûëÎèôÌïòÍ≥† ÏûàÏäµÎãàÎã§.")
        elif total_score >= 0.6:
            conclusions.append("‚ö†Ô∏è ÌïôÏäµ ÏãúÏä§ÌÖúÏù¥ Ï†ÅÏ†àÌûà ÏûëÎèôÌïòÎÇò Í∞úÏÑ† Ïó¨ÏßÄÍ∞Ä ÏûàÏäµÎãàÎã§.")
        else:
            conclusions.append("‚ùå ÌïôÏäµ ÏãúÏä§ÌÖúÏóê Ïã¨Í∞ÅÌïú Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§.")
        
        if overall_scores['learning_effectiveness'] > 0.5:
            conclusions.append("üìà ÏãúÍ∞ÑÏóê Îî∞Î•∏ ÏÑ±Îä• Í∞úÏÑ†Ïù¥ ÌôïÏù∏ÎêòÏóàÏäµÎãàÎã§.")
        
        if overall_scores['warm_vs_cold_improvement'] > 0.3:
            conclusions.append("üî• Ïõú ÏãúÏä§ÌÖúÏù¥ ÏΩúÎìú ÏãúÏä§ÌÖúÎ≥¥Îã§ ÌòÑÏ†ÄÌûà Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥ÏûÖÎãàÎã§.")
        
        if overall_scores['data_persistence'] >= 0.9:
            conclusions.append("üíæ Îç∞Ïù¥ÌÑ∞ ÏßÄÏÜçÏÑ±Ïù¥ ÌôïÏã§Ìûà Î≥¥Ïû•Îê©ÎãàÎã§.")
        
        return {
            'test_summary': {
                'total_interactions_simulated': self.learning_metrics[-1].interaction_count if self.learning_metrics else 0,
                'total_score': total_score,
                'test_passed': total_score >= 0.6,
                'conclusions': conclusions
            },
            'detailed_scores': overall_scores,
            'learning_mechanisms': learning_mechanisms,
            'learning_effectiveness': learning_effectiveness,
            'data_persistence': data_persistence,
            'cold_vs_warm_comparison': cold_vs_warm,
            'learning_analysis': learning_analysis,
            'test_environment': {
                'temp_directory': self.temp_dir,
                'user_profile': self.user_profile,
                'test_scenarios_count': len(self.scenarios)
            }
        }
    
    def cleanup(self):
        """ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω Ï†ïÎ¶¨"""
        import shutil
        try:
            shutil.rmtree(self.temp_dir, ignore_errors=True)
            print(f"üßπ ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω Ï†ïÎ¶¨ ÏôÑÎ£å: {self.temp_dir}")
        except Exception as e:
            print(f"‚ö†Ô∏è Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

def run_comprehensive_learning_test():
    """Ï¢ÖÌï© ÌïôÏäµ ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
    print("üöÄ SuperClaude ÌïôÏäµ ÏãúÏä§ÌÖú Ï¢ÖÌï© ÌÖåÏä§Ìä∏ ÏãúÏûë")
    print("=" * 80)
    
    simulator = None
    try:
        # ÏãúÎÆ¨Î†àÏù¥ÌÑ∞ Ï¥àÍ∏∞Ìôî
        simulator = LearningProgressionSimulator()
        
        # ÏÇ¨Ïö©Ïûê ÏÉÅÌò∏ÏûëÏö© ÏãúÎÆ¨Î†àÏù¥ÏÖò (25Ìöå)
        interactions = simulator.simulate_user_interactions(25)
        
        # Ï¢ÖÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        report = simulator.generate_comprehensive_report()
        
        # Î≥¥Í≥†ÏÑú Ï∂úÎ†•
        print("\n" + "=" * 80)
        print("üìä SuperClaude ÌïôÏäµ ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ Í≤∞Í≥º")
        print("=" * 80)
        
        summary = report['test_summary']
        print(f"\nüéØ ÌÖåÏä§Ìä∏ ÏöîÏïΩ:")
        print(f"   ‚Ä¢ ÏãúÎÆ¨Î†àÏù¥ÏÖòÎêú ÏÉÅÌò∏ÏûëÏö©: {summary['total_interactions_simulated']}Ìöå")
        print(f"   ‚Ä¢ Ï†ÑÏ≤¥ Ï†êÏàò: {summary['total_score']:.3f}/1.000")
        print(f"   ‚Ä¢ ÌÖåÏä§Ìä∏ ÌÜµÍ≥º: {'‚úÖ PASS' if summary['test_passed'] else '‚ùå FAIL'}")
        
        print(f"\nüìã Í≤∞Î°†:")
        for conclusion in summary['conclusions']:
            print(f"   {conclusion}")
        
        print(f"\nüìä ÏÑ∏Î∂Ä Ï†êÏàò:")
        scores = report['detailed_scores']
        for metric, score in scores.items():
            status = "‚úÖ" if score >= 0.7 else "‚ö†Ô∏è" if score >= 0.5 else "‚ùå"
            print(f"   {status} {metric}: {score:.3f}")
        
        # ÌïôÏäµ Ìö®Í≥º ÏÉÅÏÑ∏ Î∂ÑÏÑù
        if 'learning_effectiveness' in report:
            effectiveness = report['learning_effectiveness']
            print(f"\nüìà ÌïôÏäµ Ìö®Í≥º Î∂ÑÏÑù:")
            if 'improvements' in effectiveness:
                improvements = effectiveness['improvements']
                print(f"   ‚Ä¢ Ïã†Î¢∞ÎèÑ Í∞úÏÑ†: {improvements.get('confidence_improvement', 0):.2f}Ï†ê")
                print(f"   ‚Ä¢ Ï†ïÌôïÎèÑ Í∞úÏÑ†: {improvements.get('accuracy_improvement', 0):.3f}")
                print(f"   ‚Ä¢ Ìå®ÌÑ¥ Îã§ÏñëÏÑ± Ï¶ùÍ∞Ä: {improvements.get('pattern_diversity_growth', 0)}Í∞ú")
                print(f"   ‚Ä¢ ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ Í∞ïÌôî: {improvements.get('preference_strength_growth', 0):.3f}")
        
        # ÏΩúÎìú vs Ïõú ÎπÑÍµê
        if 'cold_vs_warm_comparison' in report:
            comparison = report['cold_vs_warm_comparison']
            print(f"\nüå°Ô∏è ÏΩúÎìú vs Ïõú ÏãúÏä§ÌÖú ÎπÑÍµê:")
            improvements = comparison.get('improvements', {})
            print(f"   ‚Ä¢ Ïã†Î¢∞ÎèÑ Ìñ•ÏÉÅ: {improvements.get('confidence_gain', 0):.1f}Ï†ê")
            print(f"   ‚Ä¢ ÌîåÎûòÍ∑∏ Ï†ïÍµêÌï® Ìñ•ÏÉÅ: {improvements.get('flags_sophistication_gain', 0):.1f}Í∞ú")
            print(f"   ‚Ä¢ Ï∂îÎ°† ÍπäÏù¥ Ìñ•ÏÉÅ: {improvements.get('reasoning_depth_gain', 0):.1f}Ìï≠Î™©")
        
        # Îç∞Ïù¥ÌÑ∞ ÏßÄÏÜçÏÑ±
        if 'data_persistence' in report:
            persistence = report['data_persistence']
            print(f"\nüíæ Îç∞Ïù¥ÌÑ∞ ÏßÄÏÜçÏÑ±:")
            consistency = persistence.get('data_consistency', {})
            for key, value in consistency.items():
                status = "‚úÖ" if value else "‚ùå"
                print(f"   {status} {key}: {'Î≥¥Ï°¥Îê®' if value else 'ÏÜêÏã§Îê®'}")
        
        print("\n" + "=" * 80)
        
        # JSON Î≥¥Í≥†ÏÑú Ï†ÄÏû•
        report_file = f"/tmp/superclaude_learning_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú Ï†ÄÏû•Îê®: {report_file}")
        
        return report
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        if simulator:
            simulator.cleanup()

if __name__ == "__main__":
    run_comprehensive_learning_test()