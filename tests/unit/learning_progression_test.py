#!/usr/bin/env python3
"""
SuperClaude Learning Progression Simulation and Test Suite
í•™ìŠµ ì‹œìŠ¤í…œì˜ ì‹¤ì œ í•™ìŠµ ëŠ¥ë ¥ê³¼ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ê°œì„  íš¨ê³¼ ê²€ì¦

This comprehensive test simulates 20-30 user interactions over time to verify:
1. Pattern Recognition Learning
2. User Preference Adaptation
3. Context Awareness Improvement
4. Confidence Calibration Enhancement
5. Feedback Processing Effectiveness
"""

import os
import sys
import json
import random
import sqlite3
import tempfile
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import numpy as np

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from learning_engine import AdaptiveLearningEngine, RecommendationScore
from learning_storage import LearningStorage, UserInteraction, FeedbackRecord

@dataclass
class SimulationScenario:
    """ì‹œë®¬ë ˆì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤"""
    name: str
    command: str
    description: str
    project_context: Dict[str, Any]
    expected_pattern: str
    success_probability: float
    execution_time_range: Tuple[float, float]
    user_rating_bias: float  # -1.0 to 1.0

@dataclass
class LearningMetrics:
    """í•™ìŠµ ì¸¡ì • ì§€í‘œ"""
    timestamp: str
    interaction_count: int
    avg_confidence: float
    recommendation_accuracy: float
    pattern_diversity: int
    user_preference_strength: float
    context_adaptation_score: float

class LearningProgressionSimulator:
    """í•™ìŠµ ì§„í–‰ ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self, temp_dir: str = None):
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ í…ŒìŠ¤íŠ¸
        self.temp_dir = temp_dir or tempfile.mkdtemp(prefix="superclaude_learning_test_")
        print(f"ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •: {self.temp_dir}")
        
        # í…ŒìŠ¤íŠ¸ìš© í•™ìŠµ ì‹œìŠ¤í…œ ìƒì„±
        self.storage = LearningStorage(self.temp_dir)
        self.engine = AdaptiveLearningEngine(self.storage)
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
        self.scenarios = self._create_simulation_scenarios()
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì‹œë®¬ë ˆì´ì…˜
        self.user_profile = {
            'security_focus': 0.8,  # ë³´ì•ˆì— ê´€ì‹¬ ë§ìŒ
            'performance_conscious': 0.6,  # ì„±ëŠ¥ì— ê´€ì‹¬ ë³´í†µ
            'frontend_preference': 0.3,  # í”„ë¡ íŠ¸ì—”ë“œ ì‘ì—… ì ìŒ
            'backend_preference': 0.9,  # ë°±ì—”ë“œ ì‘ì—… ë§ìŒ
            'quality_focused': 0.7,  # í’ˆì§ˆì— ê´€ì‹¬ ë§ìŒ
        }
        
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        self.project_contexts = self._create_project_contexts()
        
        # í•™ìŠµ ì§„í–‰ ë©”íŠ¸ë¦­
        self.learning_metrics: List[LearningMetrics] = []
        
        # ì´ˆê¸° ìƒíƒœ ê¸°ë¡
        self._record_initial_metrics()
    
    def _create_simulation_scenarios(self) -> List[SimulationScenario]:
        """ì‹œë®¬ë ˆì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        return [
            # ë³´ì•ˆ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="security_vulnerability_analysis",
                command="analyze",
                description="Find security vulnerabilities in authentication system",
                project_context={"languages": ["python"], "frameworks": ["django"], "project_size": "large"},
                expected_pattern="analyze_security",
                success_probability=0.85,
                execution_time_range=(45, 90),
                user_rating_bias=0.3
            ),
            SimulationScenario(
                name="api_security_audit",
                command="analyze",
                description="Security audit for REST API endpoints",
                project_context={"languages": ["javascript"], "frameworks": ["express"], "project_size": "medium"},
                expected_pattern="analyze_security",
                success_probability=0.80,
                execution_time_range=(30, 60),
                user_rating_bias=0.2
            ),
            
            # ì„±ëŠ¥ ìµœì í™” ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="database_performance_optimization",
                command="improve",
                description="Optimize database query performance",
                project_context={"languages": ["python"], "frameworks": ["sqlalchemy"], "project_size": "large"},
                expected_pattern="improve_performance",
                success_probability=0.75,
                execution_time_range=(60, 120),
                user_rating_bias=0.1
            ),
            SimulationScenario(
                name="frontend_performance_tuning",
                command="improve",
                description="Improve React component rendering performance",
                project_context={"languages": ["javascript"], "frameworks": ["react"], "project_size": "medium"},
                expected_pattern="improve_performance",
                success_probability=0.70,
                execution_time_range=(40, 80),
                user_rating_bias=-0.1
            ),
            
            # UI êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="react_component_implementation",
                command="implement",
                description="Create responsive dashboard component",
                project_context={"languages": ["javascript"], "frameworks": ["react"], "project_size": "medium"},
                expected_pattern="implement_ui",
                success_probability=0.90,
                execution_time_range=(25, 50),
                user_rating_bias=0.0
            ),
            SimulationScenario(
                name="vue_form_component",
                command="implement",
                description="Build complex form component with validation",
                project_context={"languages": ["javascript"], "frameworks": ["vue"], "project_size": "small"},
                expected_pattern="implement_ui",
                success_probability=0.85,
                execution_time_range=(30, 60),
                user_rating_bias=0.1
            ),
            
            # API êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="rest_api_implementation",
                command="implement",
                description="Implement user authentication API endpoints",
                project_context={"languages": ["python"], "frameworks": ["fastapi"], "project_size": "medium"},
                expected_pattern="implement_api",
                success_probability=0.85,
                execution_time_range=(50, 100),
                user_rating_bias=0.2
            ),
            SimulationScenario(
                name="graphql_api_implementation",
                command="implement",
                description="Create GraphQL resolvers for user management",
                project_context={"languages": ["javascript"], "frameworks": ["apollo"], "project_size": "large"},
                expected_pattern="implement_api",
                success_probability=0.80,
                execution_time_range=(60, 120),
                user_rating_bias=0.1
            ),
            
            # ì•„í‚¤í…ì²˜ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="microservices_architecture_review",
                command="analyze",
                description="Review microservices architecture design",
                project_context={"languages": ["python", "javascript"], "frameworks": ["docker", "kubernetes"], "project_size": "very_large"},
                expected_pattern="analyze_architecture",
                success_probability=0.75,
                execution_time_range=(90, 180),
                user_rating_bias=0.3
            ),
            
            # ì½”ë“œ í’ˆì§ˆ ê°œì„  ì‹œë‚˜ë¦¬ì˜¤
            SimulationScenario(
                name="code_quality_improvement",
                command="improve",
                description="Refactor legacy code for better maintainability",
                project_context={"languages": ["python"], "frameworks": [], "project_size": "large"},
                expected_pattern="improve_quality",
                success_probability=0.80,
                execution_time_range=(70, 140),
                user_rating_bias=0.2
            ),
        ]
    
    def _create_project_contexts(self) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        return [
            {
                "name": "enterprise_auth_system",
                "languages": ["python"],
                "frameworks": ["django", "celery"],
                "project_size": "very_large",
                "file_count": 150,
                "security_sensitive": True
            },
            {
                "name": "ecommerce_frontend",
                "languages": ["javascript", "typescript"],
                "frameworks": ["react", "nextjs"],
                "project_size": "large",
                "file_count": 80,
                "performance_critical": True
            },
            {
                "name": "api_gateway",
                "languages": ["python"],
                "frameworks": ["fastapi", "redis"],
                "project_size": "medium",
                "file_count": 45,
                "high_availability": True
            },
            {
                "name": "mobile_app_backend",
                "languages": ["javascript"],
                "frameworks": ["express", "mongodb"],
                "project_size": "medium",
                "file_count": 35,
                "mobile_optimized": True
            },
            {
                "name": "data_pipeline",
                "languages": ["python"],
                "frameworks": ["airflow", "pandas"],
                "project_size": "large",
                "file_count": 65,
                "data_intensive": True
            }
        ]
    
    def _record_initial_metrics(self):
        """ì´ˆê¸° ë©”íŠ¸ë¦­ ê¸°ë¡"""
        metrics = LearningMetrics(
            timestamp=datetime.now().isoformat(),
            interaction_count=0,
            avg_confidence=0.0,
            recommendation_accuracy=0.0,
            pattern_diversity=0,
            user_preference_strength=0.0,
            context_adaptation_score=0.0
        )
        self.learning_metrics.append(metrics)
    
    def simulate_user_interactions(self, num_interactions: int = 25) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì‹œë®¬ë ˆì´ì…˜"""
        print(f"\nğŸ¯ {num_interactions}ê°œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
        
        interactions = []
        
        for i in range(num_interactions):
            # ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ (ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ê°€ì¤‘ ì„ íƒ)
            scenario = self._select_scenario_by_user_preference()
            project_context = random.choice(self.project_contexts)
            
            # ì‹œê°„ ê²½ê³¼ ì‹œë®¬ë ˆì´ì…˜ (1-7ì¼ ê°„ê²©)
            if i > 0:
                time_gap = random.uniform(1, 7)  # 1-7ì¼
                time.sleep(0.1)  # ì‹¤ì œ ì‹œê°„ ê°„ê²© ì‹œë®¬ë ˆì´ì…˜
            
            print(f"  ğŸ“‹ ìƒí˜¸ì‘ìš© {i+1}/{num_interactions}: {scenario.name}")
            
            # ì¶”ì²œ ìƒì„±
            recommendation = self.engine.get_adaptive_recommendation(
                scenario.command,
                scenario.description,
                project_context
            )
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < scenario.success_probability
            execution_time = random.uniform(*scenario.execution_time_range)
            
            # ì‚¬ìš©ì í‰ì  ì‹œë®¬ë ˆì´ì…˜
            base_rating = 4 if success else 2
            rating_adjustment = scenario.user_rating_bias * random.uniform(-1, 1)
            user_rating = max(1, min(5, int(base_rating + rating_adjustment)))
            
            # ìƒí˜¸ì‘ìš© ê¸°ë¡
            interaction = UserInteraction(
                timestamp=datetime.now().isoformat(),
                user_input=f"/sc:{scenario.command} {scenario.description}",
                command=scenario.command,
                description=scenario.description,
                recommended_flags=recommendation.flags,
                actual_flags=recommendation.flags,  # ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ë™ì¼
                project_context=project_context,
                success=success,
                execution_time=execution_time,
                confidence=recommendation.confidence,
                reasoning=json.dumps(recommendation.reasoning),
                user_id=self.storage.user_id,
                project_hash=self.storage.get_project_hash(project_context['name'])
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
            interaction_id = self.storage.record_interaction(interaction)
            
            # í”¼ë“œë°± ê¸°ë¡
            feedback = FeedbackRecord(
                timestamp=datetime.now().isoformat(),
                interaction_id=interaction_id,
                feedback_type="implicit",
                rating=user_rating,
                success_indicator=success,
                user_correction=None,
                user_id=self.storage.user_id,
                project_hash=self.storage.get_project_hash(project_context['name'])
            )
            
            self.storage.record_feedback(feedback)
            
            # í•™ìŠµ ì—”ì§„ì— í”¼ë“œë°± ì ìš©
            self.engine.update_learning_from_feedback(
                interaction_id, success, execution_time, user_rating
            )
            
            # ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµ ìˆ˜í–‰ ë° ë©”íŠ¸ë¦­ ê¸°ë¡
            if (i + 1) % 5 == 0:
                print(f"    ğŸ§  í•™ìŠµ ì§„í–‰ ì¤‘... ({i+1}ê°œ ìƒí˜¸ì‘ìš© ì™„ë£Œ)")
                self.engine.learn_from_interactions(days=30)
                self._record_learning_metrics(i + 1)
            
            interactions.append({
                'scenario': scenario.name,
                'recommendation': asdict(recommendation),
                'success': success,
                'execution_time': execution_time,
                'user_rating': user_rating,
                'interaction_id': interaction_id
            })
        
        # ìµœì¢… í•™ìŠµ ìˆ˜í–‰
        print("  ğŸ“ ìµœì¢… í•™ìŠµ ìˆ˜í–‰ ì¤‘...")
        final_learning_stats = self.engine.learn_from_interactions(days=30)
        self._record_learning_metrics(num_interactions)
        
        print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {num_interactions}ê°œ ìƒí˜¸ì‘ìš©, í•™ìŠµ í†µê³„: {final_learning_stats}")
        return interactions
    
    def _select_scenario_by_user_preference(self) -> SimulationScenario:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ"""
        weights = []
        
        for scenario in self.scenarios:
            weight = 1.0  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            
            # ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
            if 'security' in scenario.name:
                weight *= (1 + self.user_profile['security_focus'])
            elif 'performance' in scenario.name:
                weight *= (1 + self.user_profile['performance_conscious'])
            elif 'ui' in scenario.name or 'component' in scenario.name:
                weight *= (1 + self.user_profile['frontend_preference'])
            elif 'api' in scenario.name:
                weight *= (1 + self.user_profile['backend_preference'])
            elif 'quality' in scenario.name:
                weight *= (1 + self.user_profile['quality_focused'])
            
            weights.append(weight)
        
        # ê°€ì¤‘ ëœë¤ ì„ íƒ
        total_weight = sum(weights)
        normalized_weights = [w/total_weight for w in weights]
        
        return np.random.choice(self.scenarios, p=normalized_weights)
    
    def _record_learning_metrics(self, interaction_count: int):
        """í•™ìŠµ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        # ìµœê·¼ ìƒí˜¸ì‘ìš© ë¶„ì„
        recent_interactions = self.storage.get_user_interactions(days=30)
        
        if not recent_interactions:
            return
        
        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        avg_confidence = np.mean([i['confidence'] for i in recent_interactions])
        
        # ì¶”ì²œ ì •í™•ë„ ê³„ì‚° (ë†’ì€ ì‹ ë¢°ë„ ì¶”ì²œì´ ì„±ê³µí–ˆëŠ”ì§€)
        high_confidence_interactions = [i for i in recent_interactions if i['confidence'] >= 80]
        if high_confidence_interactions:
            accurate_predictions = sum(1 for i in high_confidence_interactions if i['success'])
            recommendation_accuracy = accurate_predictions / len(high_confidence_interactions)
        else:
            recommendation_accuracy = 0.0
        
        # íŒ¨í„´ ë‹¤ì–‘ì„± (í•™ìŠµëœ íŒ¨í„´ ìˆ˜)
        pattern_success_rates = self.storage.get_pattern_success_rates()
        pattern_diversity = len(pattern_success_rates)
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ ê°•ë„ (ì„ í˜¸ë„ í¸ì°¨ í‰ê· )
        user_preferences = self.storage.get_user_preferences()
        if user_preferences:
            preference_values = list(user_preferences.values())
            user_preference_strength = np.std(preference_values)  # í¸ì°¨ê°€ í´ìˆ˜ë¡ ì„ í˜¸ë„ê°€ ëšœë ·í•¨
        else:
            user_preference_strength = 0.0
        
        # ì»¨í…ìŠ¤íŠ¸ ì ì‘ ì ìˆ˜ (ì»¨í…ìŠ¤íŠ¸ë³„ ì„±ê³µë¥  ì°¨ì´)
        context_adaptation_score = self._calculate_context_adaptation_score(recent_interactions)
        
        metrics = LearningMetrics(
            timestamp=datetime.now().isoformat(),
            interaction_count=interaction_count,
            avg_confidence=avg_confidence,
            recommendation_accuracy=recommendation_accuracy,
            pattern_diversity=pattern_diversity,
            user_preference_strength=user_preference_strength,
            context_adaptation_score=context_adaptation_score
        )
        
        self.learning_metrics.append(metrics)
    
    def _calculate_context_adaptation_score(self, interactions: List[Dict]) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì ì‘ ì ìˆ˜ ê³„ì‚°"""
        if len(interactions) < 10:
            return 0.0
        
        # í”„ë¡œì íŠ¸ë³„ ì„±ê³µë¥  ê³„ì‚°
        project_success_rates = {}
        for interaction in interactions:
            project_hash = interaction['project_hash']
            if project_hash not in project_success_rates:
                project_success_rates[project_hash] = []
            project_success_rates[project_hash].append(interaction['success'])
        
        # ê° í”„ë¡œì íŠ¸ì˜ ì„±ê³µë¥  ê³„ì‚°
        success_rates = []
        for project_hash, successes in project_success_rates.items():
            if len(successes) >= 3:  # ìµœì†Œ 3ê°œ ìƒ˜í”Œ
                success_rate = sum(successes) / len(successes)
                success_rates.append(success_rate)
        
        if len(success_rates) < 2:
            return 0.0
        
        # ì„±ê³µë¥  ë¶„ì‚° ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ìˆìŒ = ì¢‹ì€ ì ì‘)
        variance = np.var(success_rates)
        adaptation_score = max(0.0, 1.0 - variance * 2)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        return adaptation_score
    
    def test_learning_mechanisms(self) -> Dict[str, Any]:
        """í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”¬ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        results = {}
        
        # 1. íŒ¨í„´ ì¸ì‹ í…ŒìŠ¤íŠ¸
        print("  ğŸ“Š íŒ¨í„´ ì¸ì‹ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸")
        pattern_recognition_score = self._test_pattern_recognition()
        results['pattern_recognition'] = pattern_recognition_score
        
        # 2. ì‚¬ìš©ì ì„ í˜¸ë„ ì ì‘ í…ŒìŠ¤íŠ¸
        print("  ğŸ‘¤ ì‚¬ìš©ì ì„ í˜¸ë„ ì ì‘ í…ŒìŠ¤íŠ¸")
        user_adaptation_score = self._test_user_preference_adaptation()
        results['user_preference_adaptation'] = user_adaptation_score
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í…ŒìŠ¤íŠ¸
        print("  ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸")
        context_awareness_score = self._test_context_awareness()
        results['context_awareness'] = context_awareness_score
        
        # 4. ì‹ ë¢°ë„ ë³´ì • í…ŒìŠ¤íŠ¸
        print("  ğŸ“ˆ ì‹ ë¢°ë„ ë³´ì • ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸")
        confidence_calibration_score = self._test_confidence_calibration()
        results['confidence_calibration'] = confidence_calibration_score
        
        # 5. í”¼ë“œë°± ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("  ğŸ”„ í”¼ë“œë°± ì²˜ë¦¬ íš¨ê³¼ í…ŒìŠ¤íŠ¸")
        feedback_effectiveness_score = self._test_feedback_processing()
        results['feedback_processing'] = feedback_effectiveness_score
        
        return results
    
    def _test_pattern_recognition(self) -> float:
        """íŒ¨í„´ ì¸ì‹ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        # ì•Œë ¤ì§„ íŒ¨í„´ì— ëŒ€í•œ ì¶”ì²œ ì •í™•ë„ ì¸¡ì •
        test_cases = [
            ("analyze", "security vulnerability scan", "analyze_security"),
            ("implement", "React dashboard component", "implement_ui"),
            ("improve", "database query performance", "improve_performance"),
            ("analyze", "system architecture review", "analyze_architecture"),
        ]
        
        correct_predictions = 0
        
        for command, description, expected_pattern in test_cases:
            project_context = random.choice(self.project_contexts)
            recommendation = self.engine.get_adaptive_recommendation(command, description, project_context)
            
            # ì¶”ì²œëœ í”Œë˜ê·¸ê°€ ì˜ˆìƒ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            expected_flags = self.engine._get_pattern_base_flags(expected_pattern)
            
            # ì£¼ìš” í”Œë˜ê·¸ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            expected_flag_list = expected_flags.split()
            recommended_flag_list = recommendation.flags.split()
            
            # í•µì‹¬ í”Œë˜ê·¸ ë§¤ì¹­ í™•ì¸
            core_match = any(flag in recommended_flag_list for flag in expected_flag_list[:2])
            
            if core_match:
                correct_predictions += 1
        
        return correct_predictions / len(test_cases)
    
    def _test_user_preference_adaptation(self) -> float:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ì ì‘ í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ
        user_preferences = self.storage.get_user_preferences()
        
        if not user_preferences:
            return 0.0
        
        # ì„ í˜¸ë„ ë¶„ì‚° ì¸¡ì • (ë†’ì„ìˆ˜ë¡ ê°œì¸í™”ê°€ ì˜ ë¨)
        preference_values = list(user_preferences.values())
        preference_variance = np.var(preference_values)
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        adaptation_score = min(1.0, preference_variance)
        
        return adaptation_score
    
    def _test_context_awareness(self) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        # ë™ì¼í•œ ëª…ë ¹ì–´, ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¶”ì²œ ì°¨ì´ ì¸¡ì •
        test_command = "implement"
        test_description = "user authentication system"
        
        contexts = [
            {"languages": ["python"], "frameworks": ["django"], "project_size": "large"},
            {"languages": ["javascript"], "frameworks": ["react"], "project_size": "small"},
            {"languages": ["python"], "frameworks": ["fastapi"], "project_size": "medium"}
        ]
        
        recommendations = []
        for context in contexts:
            rec = self.engine.get_adaptive_recommendation(test_command, test_description, context)
            recommendations.append(rec.flags)
        
        # ì¶”ì²œì˜ ë‹¤ì–‘ì„± ì¸¡ì • (ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸ì— ë‹¤ë¥¸ ì¶”ì²œ)
        unique_recommendations = len(set(recommendations))
        context_awareness_score = unique_recommendations / len(contexts)
        
        return context_awareness_score
    
    def _test_confidence_calibration(self) -> float:
        """ì‹ ë¢°ë„ ë³´ì • ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        # ìµœê·¼ ìƒí˜¸ì‘ìš©ì—ì„œ ì‹ ë¢°ë„ì™€ ì‹¤ì œ ì„±ê³µë¥ ì˜ ìƒê´€ê´€ê³„ ì¸¡ì •
        recent_interactions = self.storage.get_user_interactions(days=30)
        
        if len(recent_interactions) < 10:
            return 0.0
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì‹¤ì œ ì„±ê³µë¥  ê³„ì‚°
        high_confidence = [i for i in recent_interactions if i['confidence'] >= 80]
        medium_confidence = [i for i in recent_interactions if 60 <= i['confidence'] < 80]
        low_confidence = [i for i in recent_interactions if i['confidence'] < 60]
        
        confidence_accuracy = 0.0
        weight_sum = 0.0
        
        # ê³ ì‹ ë¢°ë„ êµ¬ê°„ (80% ì´ìƒ ì„±ê³µí•´ì•¼ í•¨)
        if high_confidence:
            high_success_rate = sum(i['success'] for i in high_confidence) / len(high_confidence)
            if high_success_rate >= 0.75:  # ê¸°ëŒ€ê°’ë³´ë‹¤ ë†’ìœ¼ë©´ ì ìˆ˜
                confidence_accuracy += high_success_rate * 0.5
            weight_sum += 0.5
        
        # ì¤‘ê°„ì‹ ë¢°ë„ êµ¬ê°„ (60-70% ì„±ê³µ)
        if medium_confidence:
            medium_success_rate = sum(i['success'] for i in medium_confidence) / len(medium_confidence)
            if 0.55 <= medium_success_rate <= 0.75:  # ì ì ˆí•œ ë²”ìœ„
                confidence_accuracy += 0.65 * 0.3
            weight_sum += 0.3
        
        # ì €ì‹ ë¢°ë„ êµ¬ê°„ (50% ì´í•˜ ì„±ê³µ)
        if low_confidence:
            low_success_rate = sum(i['success'] for i in low_confidence) / len(low_confidence)
            if low_success_rate <= 0.6:  # ì˜ˆìƒëŒ€ë¡œ ë‚®ì€ ì„±ê³µë¥ 
                confidence_accuracy += (1 - low_success_rate) * 0.2
            weight_sum += 0.2
        
        return confidence_accuracy / weight_sum if weight_sum > 0 else 0.0
    
    def _test_feedback_processing(self) -> float:
        """í”¼ë“œë°± ì²˜ë¦¬ íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        # ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì„±ê³µë¥  ê°œì„  ì¸¡ì •
        all_interactions = self.storage.get_user_interactions(days=30)
        
        if len(all_interactions) < 20:
            return 0.0
        
        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_interactions = sorted(all_interactions, key=lambda x: x['timestamp'])
        
        # ì´ˆê¸° ì ˆë°˜ê³¼ í›„ë°˜ ì ˆë°˜ì˜ ì„±ê³µë¥  ë¹„êµ
        half_point = len(sorted_interactions) // 2
        
        early_interactions = sorted_interactions[:half_point]
        late_interactions = sorted_interactions[half_point:]
        
        early_success_rate = sum(i['success'] for i in early_interactions) / len(early_interactions)
        late_success_rate = sum(i['success'] for i in late_interactions) / len(late_interactions)
        
        # ê°œì„  ì •ë„ ì¸¡ì • (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
        improvement = late_success_rate - early_success_rate
        improvement_score = max(0.0, min(1.0, improvement + 0.5))  # -0.5 ~ 0.5 -> 0 ~ 1
        
        return improvement_score
    
    def measure_learning_effectiveness(self) -> Dict[str, Any]:
        """í•™ìŠµ íš¨ê³¼ ì¸¡ì •"""
        print("\nğŸ“Š í•™ìŠµ íš¨ê³¼ ì¸¡ì • ì¤‘...")
        
        if len(self.learning_metrics) < 2:
            return {"error": "ì¶©ë¶„í•œ ë©”íŠ¸ë¦­ ë°ì´í„° ì—†ìŒ"}
        
        initial_metrics = self.learning_metrics[0]
        final_metrics = self.learning_metrics[-1]
        
        # ê°œì„  ì§€í‘œ ê³„ì‚°
        improvements = {
            'confidence_improvement': final_metrics.avg_confidence - initial_metrics.avg_confidence,
            'accuracy_improvement': final_metrics.recommendation_accuracy - initial_metrics.recommendation_accuracy,
            'pattern_diversity_growth': final_metrics.pattern_diversity - initial_metrics.pattern_diversity,
            'preference_strength_growth': final_metrics.user_preference_strength - initial_metrics.user_preference_strength,
            'context_adaptation_improvement': final_metrics.context_adaptation_score - initial_metrics.context_adaptation_score
        }
        
        # ì „ì²´ í•™ìŠµ íš¨ê³¼ ì ìˆ˜
        learning_effectiveness_score = np.mean([
            max(0, improvements['confidence_improvement'] / 20),  # 20ì  ê°œì„  = 1.0ì 
            max(0, improvements['accuracy_improvement']),  # ì§ì ‘ ë¹„ìœ¨
            max(0, improvements['pattern_diversity_growth'] / 10),  # 10ê°œ íŒ¨í„´ = 1.0ì 
            max(0, improvements['preference_strength_growth']),  # ì§ì ‘ ì ìˆ˜
            max(0, improvements['context_adaptation_improvement'])  # ì§ì ‘ ì ìˆ˜
        ])
        
        return {
            'initial_state': asdict(initial_metrics),
            'final_state': asdict(final_metrics),
            'improvements': improvements,
            'learning_effectiveness_score': learning_effectiveness_score,
            'metrics_timeline': [asdict(m) for m in self.learning_metrics]
        }
    
    def test_data_persistence(self) -> Dict[str, Any]:
        """ë°ì´í„° ì§€ì†ì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ’¾ ë°ì´í„° ì§€ì†ì„± í…ŒìŠ¤íŠ¸")
        
        # í˜„ì¬ ë°ì´í„° ìƒíƒœ ê¸°ë¡
        before_state = {
            'interactions_count': len(self.storage.get_user_interactions(days=30)),
            'patterns_count': len(self.storage.get_pattern_success_rates()),
            'preferences_count': len(self.storage.get_user_preferences())
        }
        
        # ìƒˆë¡œìš´ ìŠ¤í† ë¦¬ì§€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¬ì‹œì‘ ì‹œë®¬ë ˆì´ì…˜)
        new_storage = LearningStorage(self.temp_dir)
        new_engine = AdaptiveLearningEngine(new_storage)
        
        # ë°ì´í„° ë³µêµ¬ í™•ì¸
        after_state = {
            'interactions_count': len(new_storage.get_user_interactions(days=30)),
            'patterns_count': len(new_storage.get_pattern_success_rates()),
            'preferences_count': len(new_storage.get_user_preferences())
        }
        
        # ë°ì´í„° ì¼ê´€ì„± í™•ì¸
        data_consistency = {
            'interactions_preserved': before_state['interactions_count'] == after_state['interactions_count'],
            'patterns_preserved': before_state['patterns_count'] == after_state['patterns_count'],
            'preferences_preserved': before_state['preferences_count'] == after_state['preferences_count']
        }
        
        # í•™ìŠµ ìƒíƒœ ë³µêµ¬ í…ŒìŠ¤íŠ¸
        test_recommendation = new_engine.get_adaptive_recommendation(
            "analyze", "security vulnerability assessment", 
            {"languages": ["python"], "project_size": "large"}
        )
        
        learning_state_recovered = test_recommendation.confidence > 60  # ê¸°ë³¸ê°’ë³´ë‹¤ ë†’ì•„ì•¼ í•¨
        
        return {
            'before_restart': before_state,
            'after_restart': after_state,
            'data_consistency': data_consistency,
            'learning_state_recovered': learning_state_recovered,
            'persistence_score': sum(data_consistency.values()) / len(data_consistency)
        }
    
    def compare_cold_vs_warm_performance(self) -> Dict[str, Any]:
        """ì½œë“œ ìŠ¤íƒ€íŠ¸ vs ì›œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ"""
        print("\nğŸŒ¡ï¸ ì½œë“œ ìŠ¤íƒ€íŠ¸ vs ì›œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ")
        
        # ì›œ ì‹œìŠ¤í…œ ì„±ëŠ¥ (í˜„ì¬ í•™ìŠµëœ ìƒíƒœ)
        warm_test_cases = [
            ("analyze", "security audit for web application"),
            ("implement", "REST API for user management"),
            ("improve", "database query optimization"),
        ]
        
        warm_performance = []
        for command, description in warm_test_cases:
            context = random.choice(self.project_contexts)
            recommendation = self.engine.get_adaptive_recommendation(command, description, context)
            warm_performance.append({
                'confidence': recommendation.confidence,
                'flags_count': len(recommendation.flags.split()),
                'reasoning_depth': len(recommendation.reasoning)
            })
        
        # ì½œë“œ ì‹œìŠ¤í…œ ì„±ëŠ¥ (ìƒˆë¡œìš´ ì„ì‹œ ì‹œìŠ¤í…œ)
        cold_temp_dir = tempfile.mkdtemp(prefix="superclaude_cold_test_")
        cold_storage = LearningStorage(cold_temp_dir)
        cold_engine = AdaptiveLearningEngine(cold_storage)
        
        cold_performance = []
        for command, description in warm_test_cases:
            context = random.choice(self.project_contexts)
            recommendation = cold_engine.get_adaptive_recommendation(command, description, context)
            cold_performance.append({
                'confidence': recommendation.confidence,
                'flags_count': len(recommendation.flags.split()),
                'reasoning_depth': len(recommendation.reasoning)
            })
        
        # ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
        warm_avg_confidence = np.mean([p['confidence'] for p in warm_performance])
        cold_avg_confidence = np.mean([p['confidence'] for p in cold_performance])
        
        warm_avg_flags = np.mean([p['flags_count'] for p in warm_performance])
        cold_avg_flags = np.mean([p['flags_count'] for p in cold_performance])
        
        warm_avg_reasoning = np.mean([p['reasoning_depth'] for p in warm_performance])
        cold_avg_reasoning = np.mean([p['reasoning_depth'] for p in cold_performance])
        
        # ì •ë¦¬
        import shutil
        shutil.rmtree(cold_temp_dir, ignore_errors=True)
        
        return {
            'warm_system': {
                'avg_confidence': warm_avg_confidence,
                'avg_flags_count': warm_avg_flags,
                'avg_reasoning_depth': warm_avg_reasoning,
                'performance_samples': warm_performance
            },
            'cold_system': {
                'avg_confidence': cold_avg_confidence,
                'avg_flags_count': cold_avg_flags,
                'avg_reasoning_depth': cold_avg_reasoning,
                'performance_samples': cold_performance
            },
            'improvements': {
                'confidence_gain': warm_avg_confidence - cold_avg_confidence,
                'flags_sophistication_gain': warm_avg_flags - cold_avg_flags,
                'reasoning_depth_gain': warm_avg_reasoning - cold_avg_reasoning
            },
            'overall_improvement_score': (
                (warm_avg_confidence - cold_avg_confidence) / 100 +  # ì‹ ë¢°ë„ ê°œì„ 
                max(0, (warm_avg_flags - cold_avg_flags) / 5) +  # í”Œë˜ê·¸ ì •êµí•¨
                max(0, (warm_avg_reasoning - cold_avg_reasoning) / 10)  # ì¶”ë¡  ê¹Šì´
            ) / 3
        }
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘
        learning_mechanisms = self.test_learning_mechanisms()
        learning_effectiveness = self.measure_learning_effectiveness()
        data_persistence = self.test_data_persistence()
        cold_vs_warm = self.compare_cold_vs_warm_performance()
        
        # ìµœì¢… í•™ìŠµ ë¶„ì„
        learning_analysis = self.engine.analyze_learning_progress()
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_scores = {
            'pattern_recognition': learning_mechanisms.get('pattern_recognition', 0),
            'user_adaptation': learning_mechanisms.get('user_preference_adaptation', 0),
            'context_awareness': learning_mechanisms.get('context_awareness', 0),
            'confidence_calibration': learning_mechanisms.get('confidence_calibration', 0),
            'feedback_processing': learning_mechanisms.get('feedback_processing', 0),
            'learning_effectiveness': learning_effectiveness.get('learning_effectiveness_score', 0),
            'data_persistence': data_persistence.get('persistence_score', 0),
            'warm_vs_cold_improvement': cold_vs_warm.get('overall_improvement_score', 0)
        }
        
        total_score = np.mean(list(overall_scores.values()))
        
        # ê²°ë¡  ë„ì¶œ
        conclusions = []
        if total_score >= 0.8:
            conclusions.append("âœ… í•™ìŠµ ì‹œìŠ¤í…œì´ ë§¤ìš° íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        elif total_score >= 0.6:
            conclusions.append("âš ï¸ í•™ìŠµ ì‹œìŠ¤í…œì´ ì ì ˆíˆ ì‘ë™í•˜ë‚˜ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            conclusions.append("âŒ í•™ìŠµ ì‹œìŠ¤í…œì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        
        if overall_scores['learning_effectiveness'] > 0.5:
            conclusions.append("ğŸ“ˆ ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ ê°œì„ ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if overall_scores['warm_vs_cold_improvement'] > 0.3:
            conclusions.append("ğŸ”¥ ì›œ ì‹œìŠ¤í…œì´ ì½œë“œ ì‹œìŠ¤í…œë³´ë‹¤ í˜„ì €íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        
        if overall_scores['data_persistence'] >= 0.9:
            conclusions.append("ğŸ’¾ ë°ì´í„° ì§€ì†ì„±ì´ í™•ì‹¤íˆ ë³´ì¥ë©ë‹ˆë‹¤.")
        
        return {
            'test_summary': {
                'total_interactions_simulated': self.learning_metrics[-1].interaction_count if self.learning_metrics else 0,
                'total_score': total_score,
                'test_passed': total_score >= 0.6,
                'conclusions': conclusions
            },
            'detailed_scores': overall_scores,
            'learning_mechanisms': learning_mechanisms,
            'learning_effectiveness': learning_effectiveness,
            'data_persistence': data_persistence,
            'cold_vs_warm_comparison': cold_vs_warm,
            'learning_analysis': learning_analysis,
            'test_environment': {
                'temp_directory': self.temp_dir,
                'user_profile': self.user_profile,
                'test_scenarios_count': len(self.scenarios)
            }
        }
    
    def cleanup(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬"""
        import shutil
        try:
            shutil.rmtree(self.temp_dir, ignore_errors=True)
            print(f"ğŸ§¹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ: {self.temp_dir}")
        except Exception as e:
            print(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

def run_comprehensive_learning_test():
    """ì¢…í•© í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ SuperClaude í•™ìŠµ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    simulator = None
    try:
        # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
        simulator = LearningProgressionSimulator()
        
        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì‹œë®¬ë ˆì´ì…˜ (25íšŒ)
        interactions = simulator.simulate_user_interactions(25)
        
        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        report = simulator.generate_comprehensive_report()
        
        # ë³´ê³ ì„œ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š SuperClaude í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 80)
        
        summary = report['test_summary']
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        print(f"   â€¢ ì‹œë®¬ë ˆì´ì…˜ëœ ìƒí˜¸ì‘ìš©: {summary['total_interactions_simulated']}íšŒ")
        print(f"   â€¢ ì „ì²´ ì ìˆ˜: {summary['total_score']:.3f}/1.000")
        print(f"   â€¢ í…ŒìŠ¤íŠ¸ í†µê³¼: {'âœ… PASS' if summary['test_passed'] else 'âŒ FAIL'}")
        
        print(f"\nğŸ“‹ ê²°ë¡ :")
        for conclusion in summary['conclusions']:
            print(f"   {conclusion}")
        
        print(f"\nğŸ“Š ì„¸ë¶€ ì ìˆ˜:")
        scores = report['detailed_scores']
        for metric, score in scores.items():
            status = "âœ…" if score >= 0.7 else "âš ï¸" if score >= 0.5 else "âŒ"
            print(f"   {status} {metric}: {score:.3f}")
        
        # í•™ìŠµ íš¨ê³¼ ìƒì„¸ ë¶„ì„
        if 'learning_effectiveness' in report:
            effectiveness = report['learning_effectiveness']
            print(f"\nğŸ“ˆ í•™ìŠµ íš¨ê³¼ ë¶„ì„:")
            if 'improvements' in effectiveness:
                improvements = effectiveness['improvements']
                print(f"   â€¢ ì‹ ë¢°ë„ ê°œì„ : {improvements.get('confidence_improvement', 0):.2f}ì ")
                print(f"   â€¢ ì •í™•ë„ ê°œì„ : {improvements.get('accuracy_improvement', 0):.3f}")
                print(f"   â€¢ íŒ¨í„´ ë‹¤ì–‘ì„± ì¦ê°€: {improvements.get('pattern_diversity_growth', 0)}ê°œ")
                print(f"   â€¢ ì‚¬ìš©ì ì„ í˜¸ë„ ê°•í™”: {improvements.get('preference_strength_growth', 0):.3f}")
        
        # ì½œë“œ vs ì›œ ë¹„êµ
        if 'cold_vs_warm_comparison' in report:
            comparison = report['cold_vs_warm_comparison']
            print(f"\nğŸŒ¡ï¸ ì½œë“œ vs ì›œ ì‹œìŠ¤í…œ ë¹„êµ:")
            improvements = comparison.get('improvements', {})
            print(f"   â€¢ ì‹ ë¢°ë„ í–¥ìƒ: {improvements.get('confidence_gain', 0):.1f}ì ")
            print(f"   â€¢ í”Œë˜ê·¸ ì •êµí•¨ í–¥ìƒ: {improvements.get('flags_sophistication_gain', 0):.1f}ê°œ")
            print(f"   â€¢ ì¶”ë¡  ê¹Šì´ í–¥ìƒ: {improvements.get('reasoning_depth_gain', 0):.1f}í•­ëª©")
        
        # ë°ì´í„° ì§€ì†ì„±
        if 'data_persistence' in report:
            persistence = report['data_persistence']
            print(f"\nğŸ’¾ ë°ì´í„° ì§€ì†ì„±:")
            consistency = persistence.get('data_consistency', {})
            for key, value in consistency.items():
                status = "âœ…" if value else "âŒ"
                print(f"   {status} {key}: {'ë³´ì¡´ë¨' if value else 'ì†ì‹¤ë¨'}")
        
        print("\n" + "=" * 80)
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        report_file = f"/tmp/superclaude_learning_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ë¨: {report_file}")
        
        return report
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        if simulator:
            simulator.cleanup()

if __name__ == "__main__":
    run_comprehensive_learning_test()